{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a differentially private LSTM model for name classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will build a differentially-private LSTM model to classify names to their source languages, which is the same task as in the tutorial **NLP From Scratch** (https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html). Since the objective of this tutorial is to demonstrate the effective use of an LSTM with privacy guarantees, we will be utilizing it in place of the bare-bones RNN model defined in the original tutorial. Specifically, we use the `DPLSTM` module from `opacus.layers.dp_lstm` to facilitate the calculation of the per-example gradients, which are utilized in the addition of noise during the application of differential privacy. `DPLSTM` has the same API and functionality as the `nn.LSTM`, with some restrictions (ex. we currently support single layers, the full list is given below).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us download the dataset of names and their associated language labels as given in https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html. We train our differentially-private LSTM on the same dataset as in that tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "NAMES_DATASET_URL = \"https://download.pytorch.org/tutorial/data.zip\"\n",
    "DATA_DIR = \"names\"\n",
    "\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "def download_and_extract(dataset_url, data_dir):\n",
    "    print(\"Downloading and extracting ...\")\n",
    "    filename = \"data.zip\"\n",
    "\n",
    "    urllib.request.urlretrieve(dataset_url, filename)\n",
    "    with zipfile.ZipFile(filename) as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(filename)\n",
    "    print(\"Completed!\")\n",
    "\n",
    "download_and_extract(NAMES_DATASET_URL, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_folder = os.path.join(DATA_DIR, 'data', 'names')\n",
    "all_filenames = []\n",
    "\n",
    "for language_file in os.listdir(names_folder):\n",
    "    all_filenames.append(os.path.join(names_folder, language_file))\n",
    "    \n",
    "print(os.listdir(names_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharByteEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This encoder takes a UTF-8 string and encodes its bytes into a Tensor. It can also\n",
    "    perform the opposite operation to check a result.\n",
    "    Examples:\n",
    "    >>> encoder = CharByteEncoder()\n",
    "    >>> t = encoder('Ślusàrski')  # returns tensor([256, 197, 154, 108, 117, 115, 195, 160, 114, 115, 107, 105, 257])\n",
    "    >>> encoder.decode(t)  # returns \"<s>Ślusàrski</s>\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.start_token = \"<s>\"\n",
    "        self.end_token = \"</s>\"\n",
    "        self.pad_token = \"<pad>\"\n",
    "\n",
    "        self.start_idx = 256\n",
    "        self.end_idx = 257\n",
    "        self.pad_idx = 258\n",
    "\n",
    "    def forward(self, s: str, pad_to=0) -> torch.LongTensor:\n",
    "        \"\"\"\n",
    "        Encodes a string. It will append a start token <s> (id=self.start_idx) and an end token </s>\n",
    "        (id=self.end_idx).\n",
    "        Args:\n",
    "            s: The string to encode.\n",
    "            pad_to: If not zero, pad by appending self.pad_idx until string is of length `pad_to`.\n",
    "                Defaults to 0.\n",
    "        Returns:\n",
    "            The encoded LongTensor of indices.\n",
    "        \"\"\"\n",
    "        encoded = s.encode()\n",
    "        n_pad = pad_to - len(encoded) if pad_to > len(encoded) else 0\n",
    "        return torch.LongTensor(\n",
    "            [self.start_idx]\n",
    "            + [c for c in encoded]  # noqa\n",
    "            + [self.end_idx]\n",
    "            + [self.pad_idx for _ in range(n_pad)]\n",
    "        )\n",
    "\n",
    "    def decode(self, char_ids_tensor: torch.LongTensor) -> str:\n",
    "        \"\"\"\n",
    "        The inverse of `forward`. Keeps the start, end, and pad indices.\n",
    "        \"\"\"\n",
    "        char_ids = char_ids_tensor.cpu().detach().tolist()\n",
    "\n",
    "        out = []\n",
    "        buf = []\n",
    "        for c in char_ids:\n",
    "            if c < 256:\n",
    "                buf.append(c)\n",
    "            else:\n",
    "                if buf:\n",
    "                    out.append(bytes(buf).decode())\n",
    "                    buf = []\n",
    "                if c == self.start_idx:\n",
    "                    out.append(self.start_token)\n",
    "                elif c == self.end_idx:\n",
    "                    out.append(self.end_token)\n",
    "                elif c == self.pad_idx:\n",
    "                    out.append(self.pad_token)\n",
    "\n",
    "        if buf:  # in case some are left\n",
    "            out.append(bytes(buf).decode())\n",
    "        return \"\".join(out)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The length of our encoder space. This is fixed to 256 (one byte) + 3 special chars\n",
    "        (start, end, pad).\n",
    "        Returns:\n",
    "            259\n",
    "        \"\"\"\n",
    "        return 259"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Validation Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def padded_collate(batch, padding_idx=0):\n",
    "    x = pad_sequence(\n",
    "        [elem[0] for elem in batch], batch_first=True, padding_value=padding_idx\n",
    "    )\n",
    "    y = torch.stack([elem[1] for elem in batch]).long()\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = Path(root)\n",
    "\n",
    "        self.labels = list({langfile.stem for langfile in self.root.iterdir()})\n",
    "        self.labels_dict = {label: i for i, label in enumerate(self.labels)}\n",
    "        self.encoder = CharByteEncoder()\n",
    "        self.samples = self.construct_samples()\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.samples[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def construct_samples(self):\n",
    "        samples = []\n",
    "        for langfile in self.root.iterdir():\n",
    "            label_name = langfile.stem\n",
    "            label_id = self.labels_dict[label_name]\n",
    "            with open(langfile, \"r\") as fin:\n",
    "                for row in fin:\n",
    "                    samples.append(\n",
    "                        (self.encoder(row.strip()), torch.tensor(label_id).long())\n",
    "                    )\n",
    "        return samples\n",
    "\n",
    "    def label_count(self):\n",
    "        cnt = Counter()\n",
    "        for _x, y in self.samples:\n",
    "            label = self.labels[int(y)]\n",
    "            cnt[label] += 1\n",
    "        return cnt\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 256 + 3  # 256 alternatives in one byte, plus 3 special characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into a 80-20 split for training and validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secure_mode = False\n",
    "train_split = 0.8\n",
    "test_every = 5\n",
    "batch_size = 800\n",
    "\n",
    "ds = NamesDataset(names_folder)\n",
    "train_len = int(train_split * len(ds))\n",
    "test_len = len(ds) - train_len\n",
    "\n",
    "print(f\"{train_len} samples for training, {test_len} for testing\")\n",
    "\n",
    "train_ds, test_ds = torch.utils.data.random_split(ds, [train_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=padded_collate,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=2 * batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    collate_fn=padded_collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the dataset into a training and a validation set, we now have to convert the data into a numeric form suitable for training the LSTM model. For each name, we set a maximum sequence length of 15, and if a name is longer than the threshold, we truncate it (this rarely happens in this dataset!). If a name is smaller than the threshold, we add a dummy `#` character to pad it to the desired length. We also batch the names in the dataset and set a batch size of 256 for all the experiments in this tutorial. The function `line_to_tensor()` returns a tensor of shape [15, 256] where each element is the index (in `all_letters`) of the corresponding character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Evaluation Cycle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and the evaluation functions `train()` and `test()` are defined below. During the training loop, the per-example gradients are computed and the parameters are updated subsequent to gradient clipping (to bound their sensitivity) and addition of noise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, epoch, privacy_engine, device=\"cuda:0\"):\n",
    "    accs = []\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = logits.argmax(-1)\n",
    "        n_correct = float(preds.eq(y).sum())\n",
    "        batch_accuracy = n_correct / len(y)\n",
    "\n",
    "        accs.append(batch_accuracy)\n",
    "        losses.append(float(loss))\n",
    "\n",
    "    printstr = (\n",
    "        f\"\\t Epoch {epoch}. Accuracy: {mean(accs):.6f} | Loss: {mean(losses):.6f}\"\n",
    "    )\n",
    "    if privacy_engine:\n",
    "        epsilon = privacy_engine.get_epsilon(delta)\n",
    "        printstr += f\" | (ε = {epsilon:.2f}, δ = {delta})\"\n",
    "\n",
    "    print(printstr)\n",
    "    return\n",
    "\n",
    "\n",
    "def test(model, test_loader, privacy_engine, device=\"cuda:0\"):\n",
    "    accs = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            preds = model(x).argmax(-1)\n",
    "            n_correct = float(preds.eq(y).sum())\n",
    "            batch_accuracy = n_correct / len(y)\n",
    "\n",
    "            accs.append(batch_accuracy)\n",
    "    printstr = \"\\n----------------------------\\n\" f\"Test Accuracy: {mean(accs):.6f}\"\n",
    "    if privacy_engine:\n",
    "        epsilon = privacy_engine.get_epsilon(delta)\n",
    "        printstr += f\" (ε = {epsilon:.2f}, δ = {delta})\"\n",
    "    print(printstr + \"\\n----------------------------\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two sets of hyper-parameters associated with this model. The first are hyper-parameters which we would expect in any machine learning training, such as the learning rate and batch size. The second set are related to the privacy engine, where for example we define the amount of noise added to the gradients (`noise_multiplier`), and the maximum L2 norm to which the per-sample gradients are clipped (`max_grad_norm`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyper-parameters\n",
    "epochs = 50\n",
    "learning_rate = 2.0\n",
    "\n",
    "# Privacy engine hyper-parameters\n",
    "max_per_sample_grad_norm = 1.5\n",
    "delta = 8e-5\n",
    "epsilon = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the name classification model in the cell below. Note that it is a simple char-LSTM classifier, where the input characters are passed through an `nn.Embedding` layer, and are subsequently input to the DPLSTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from opacus.layers import DPLSTM\n",
    "\n",
    "class CharNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        hidden_size,\n",
    "        output_size,\n",
    "        num_lstm_layers=1,\n",
    "        bidirectional=False,\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = DPLSTM(\n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            num_layers=num_lstm_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.out_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)  # -> [B, T, D]\n",
    "        x, _ = self.lstm(x, hidden)  # -> [B, T, H]\n",
    "        x = x[:, -1, :]  # -> [B, H]\n",
    "        x = self.out_layer(x)  # -> [B, C]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to instantiate the objects (privacy engine, model and optimizer) for our differentially-private LSTM training.  However, the `nn.LSTM` is replaced with a `DPLSTM` module which enables us to calculate per-example gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device to run on a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define classifier parameters\n",
    "embedding_size = 64\n",
    "hidden_size = 128  # Number of neurons in hidden layer after LSTM\n",
    "n_lstm_layers = 1\n",
    "bidirectional_lstm = False\n",
    "\n",
    "model = CharNNClassifier(\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    len(ds.labels),\n",
    "    n_lstm_layers,\n",
    "    bidirectional_lstm,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the privacy engine, optimizer and loss criterion for the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opacus import PrivacyEngine\n",
    "privacy_engine = PrivacyEngine(secure_mode=secure_mode)\n",
    "\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    max_grad_norm=max_per_sample_grad_norm,\n",
    "    target_delta=delta,\n",
    "    target_epsilon=epsilon,\n",
    "    epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the name classifier with privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training! We will be training for 50 epochs iterations (where each epoch corresponds to a pass over the whole dataset). We will be reporting the privacy epsilon every `test_every` epoch. We will also benchmark this differentially-private model against a model without privacy and obtain almost identical performance. Further, the private model trained with Opacus incurs only minimal overhead in training time, with the differentially-private classifier only slightly slower (by a couple of minutes) than the non-private model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train stats: \\n\")\n",
    "for epoch in range(epochs):\n",
    "    train(model, criterion, optimizer, train_loader, epoch, privacy_engine, device=device)\n",
    "    if test_every:\n",
    "        if epoch % test_every == 0:\n",
    "            test(model, test_loader, privacy_engine, device=device)\n",
    "\n",
    "test(model, test_loader, privacy_engine, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The differentially-private name classification model obtains a test accuracy of 0.75 with an epsilon of just under 12. This shows that we can achieve good accuracy on this task, with minimal loss of privacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the name classifier without privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We also run a comparison with a non-private model to see if the performance obtained with privacy is comparable to it. To do this, we keep the parameters such as learning rate and batch size the same, and only define a different instance of the model along with a separate optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nodp = CharNNClassifier(\n",
    "    embedding_size,\n",
    "    hidden_size,\n",
    "    len(ds.labels),\n",
    "    n_lstm_layers,\n",
    "    bidirectional_lstm,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "optimizer_nodp = torch.optim.SGD(model_nodp.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(model_nodp, criterion, optimizer_nodp, train_loader, epoch, device=device)\n",
    "    if test_every:\n",
    "        if epoch % test_every == 0:\n",
    "            test(model_nodp, test_loader, None, device=device)\n",
    "\n",
    "test(model_nodp, test_loader, None, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the training loop again, this time without privacy and for the same number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-private classifier obtains a test accuracy of around 0.75 with the same parameters and number of epochs. We are effectively trading off performance on the name classification task for a lower loss of privacy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
